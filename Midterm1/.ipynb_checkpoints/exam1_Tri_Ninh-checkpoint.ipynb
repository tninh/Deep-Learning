{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('exam1_train.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(X, test_ratio):\n",
    "    np.random.seed(1)\n",
    "    shuffled_indices = np.random.permutation(len(X))\n",
    "    test_set_size = int(len(X) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return X.iloc[train_indices], X.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df.iloc[:,400]\n",
    "x_train = df.iloc[:,0:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(mat):\n",
    "    mat = mat.as_matrix()\n",
    "    labels = []\n",
    "    for e in mat:\n",
    "        if e not in labels:\n",
    "            labels.append(e)\n",
    "    labels.sort()\n",
    "    \n",
    "    enc = np.zeros(shape=(len(mat),len(labels)), dtype=int)\n",
    "    #print result[0,0]\n",
    "    for key, val in enumerate(mat):\n",
    "        enc[key][labels.index(val)] = 1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = one_hot_encoding(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_encoded = pd.DataFrame(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    A = 1. / (1 + np.exp(-z))\n",
    "#     z_temp = z\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    A = np.maximum(0,x)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    assert(A2.shape == (W2.shape[0], A1.shape[1]))\n",
    "    \n",
    "    cache_variables = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2, \"Z3\": Z3, \"A3\":A3}\n",
    "\n",
    "    return A3, cache_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_dropout_regularization(X, parameters, keep_prob):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    \n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = relu(Z1)\n",
    "             \n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1])     \n",
    "    D1 = D1 < keep_prob                      \n",
    "    A1 = A1 * D1                                      \n",
    "    A1 = A1 / keep_prob                               \n",
    "    \n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    D2 = np.random.rand(A2.shape[0], A2.shape[1])     \n",
    "    D2 = D2 < keep_prob                                   \n",
    "    A2 = A2 * D2                                    \n",
    "    A2 = A2 / keep_prob                               \n",
    "    \n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache_variables = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation_dropout_regularization(X, Y, cache, keep_prob):\n",
    "   \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dZ3 = np.array(dZ3)\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    \n",
    "    dA2 = dA2 * D2              \n",
    "    dA2 = dA2 / keep_prob              \n",
    "    \n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T)\n",
    "    dZ2 = np.array(dZ2)\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    \n",
    "    dA1 = dA1 * D1             \n",
    "    dA1 = dA1 / keep_prob             \n",
    "    \n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    dZ1 = np.array(dZ1)\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_scores = np.array(np.exp(x))\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    y_predict = np.array(np.argmax(probs, axis=0))\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "- Since the data is already splitted when you provided to us, and for the sake of better accuracy, I don't use the split data function to split the training set but use the whole training set to train and test with the whole test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_size):    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_size) \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_size[l], layer_size[l-1]) / np.sqrt(layer_size[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_size[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cost(A3, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    logprobs = np.log(A3) * Y + (1 - Y) * np.log(1 - A3)\n",
    "    cost = - np.nansum(logprobs) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function below equals to gradient_descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \n",
    "    dW1 = gradients['dW1']\n",
    "    db1 = gradients['db1']\n",
    "    dW2 = gradients['dW2']\n",
    "    db2 = gradients['db2']\n",
    "    dW3 = gradients['dW3']\n",
    "    db3 = gradients['db3']\n",
    "   \n",
    "    parameters[\"W1\"] = parameters[\"W1\"] - 0.9 * dW1\n",
    "    parameters[\"b1\"] = parameters[\"b1\"] - 0.9 * db1\n",
    "    parameters[\"W2\"] = parameters[\"W2\"] - 0.9 * dW2\n",
    "    parameters[\"b2\"] = parameters[\"b2\"] - 0.9 * db2\n",
    "    parameters[\"W3\"] = parameters[\"W3\"] - learning_rate * dW3\n",
    "    parameters[\"b3\"] = parameters[\"b3\"] - learning_rate * db3\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define fit function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X, Y, learning_rate = 0.3, num_iterations = 8000, n_h_1=390, n_h_2=390, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    grads = {}\n",
    "    costs = []                           \n",
    "    m = X.shape[1]                        \n",
    "    layers_sizes = [X.shape[0], n_h_1, n_h_2, 10]\n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameters(layers_sizes)\n",
    "    \n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        if keep_prob < 1:\n",
    "            a3, cache = forward_propagation_dropout_regularization(X, parameters, keep_prob)\n",
    "            cost = calculate_cost(a3, Y)\n",
    "            \n",
    "        assert(lambd == 0 or keep_prob == 1)   \n",
    "        if keep_prob < 1:\n",
    "            gradients = backward_propagation_dropout_regularization(X, Y, cache, keep_prob)\n",
    "        \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after {} iterations: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    A3, cache = forward_propagation(X, parameters)\n",
    "    #print cache[10].shape\n",
    "    y_predict = softmax(cache[\"Z3\"])\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized model_fit or Optimization\n",
    "- In this optimization, the accuracy for the test set in my machine (Macbook Pro 13, Intel i5 Dual Cores, 2.5GHz) is __90.8%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 6.98663719482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trininh/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 1000 iterations: 0.00284356224986\n",
      "Cost after 2000 iterations: 0.000937536490227\n",
      "Cost after 3000 iterations: 0.000524399421809\n",
      "Cost after 4000 iterations: 0.000353530959333\n",
      "Cost after 5000 iterations: 0.000262536046163\n",
      "Cost after 6000 iterations: 0.000206823446946\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = fit(x_train.T, y_train_encoded.T, keep_prob=0.89, n_h_1 = 150, n_h_2 = 100, learning_rate=1.12, num_iterations=7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = predict(parameters, x_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized train set accuracy = 95.3142857143%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(np.array(y_train), predictions)]  \n",
    "accuracy = (float(sum(map(int, correct))) / float(len(correct)))  \n",
    "print 'Optimized train set accuracy = {0}%'.format(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('exam1_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = df1.iloc[:,0:400]\n",
    "y_test = df1.iloc[:,400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized test set accuracy = 90.8%\n"
     ]
    }
   ],
   "source": [
    "predictions1 = predict(parameters, x_test.T)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_test, predictions1)]  \n",
    "accuracy = (float(sum(map(int, correct))) / float(len(correct)))  \n",
    "print 'Optimized test set accuracy = {0}%'.format(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: Please Let the Optimization above finish running and then run this. \n",
    "### Other Optimization\n",
    "- In this optimization, I will just apply dropout regularization for layer #2, the accuracy for the test set in my machine (Macbook Pro 13, Intel i5 Dual Cores, 2.5GHz) is __90.8%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_dropout_regularization_layer2(X, parameters, keep_prob):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    \n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = relu(Z1)\n",
    "             \n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1])     \n",
    "#     D1 = D1 < keep_prob                      \n",
    "#     A1 = A1 * D1                                      \n",
    "#     A1 = A1 / keep_prob                               \n",
    "    \n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    \n",
    "    D2 = np.random.rand(A2.shape[0], A2.shape[1])     \n",
    "    D2 = D2 < keep_prob                                   \n",
    "    A2 = A2 * D2                                    \n",
    "    A2 = A2 / keep_prob                               \n",
    "    \n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache_variables = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation_dropout_regularization_layer2(X, Y, cache, keep_prob):\n",
    "   \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dZ3 = np.array(dZ3)\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    \n",
    "    dA2 = dA2 * D2              \n",
    "    dA2 = dA2 / keep_prob              \n",
    "    \n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T)\n",
    "    dZ2 = np.array(dZ2)\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    \n",
    "#     dA1 = dA1 * D1             \n",
    "#     dA1 = dA1 / keep_prob             \n",
    "    \n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    dZ1 = np.array(dZ1)\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_reg_layer2(X, Y, learning_rate = 0.3, num_iterations = 8000, n_h_1=390, n_h_2=390, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    grads = {}\n",
    "    costs = []                           \n",
    "    m = X.shape[1]                        \n",
    "    layers_sizes = [X.shape[0], n_h_1, n_h_2, 10]\n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameters(layers_sizes)\n",
    "    \n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        if keep_prob < 1:\n",
    "            a3, cache = forward_propagation_dropout_regularization_layer2(X, parameters, keep_prob)\n",
    "            cost = calculate_cost(a3, Y)\n",
    "            \n",
    "        assert(lambd == 0 or keep_prob == 1)   \n",
    "        if keep_prob < 1:\n",
    "            gradients = backward_propagation_dropout_regularization_layer2(X, Y, cache, keep_prob)\n",
    "        \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after {} iterations: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 7.01204288618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trininh/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 1000 iterations: 0.00924788541855\n",
      "Cost after 2000 iterations: 0.00288899908325\n",
      "Cost after 3000 iterations: 0.00138091592438\n",
      "Cost after 4000 iterations: 0.000850495800032\n",
      "Cost after 5000 iterations: 0.000598966141862\n",
      "Cost after 6000 iterations: 0.000455474906956\n"
     ]
    }
   ],
   "source": [
    "parameters1, costs1 = fit_reg_layer2(x_train.T, y_train_encoded.T, keep_prob=0.89, n_h_1 = 150, n_h_2 = 20, learning_rate=1.12, num_iterations=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized train set accuracy = 98.3428571429%\n"
     ]
    }
   ],
   "source": [
    "predictions2 = predict(parameters1, x_train.T)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(np.array(y_train), predictions2)]  \n",
    "accuracy = (float(sum(map(int, correct))) / float(len(correct)))  \n",
    "print 'Optimized train set accuracy = {0}%'.format(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized test set accuracy = 90.8%\n"
     ]
    }
   ],
   "source": [
    "predictions3 = predict(parameters1, x_test.T)\n",
    "correct = [1 if a == b else 0 for (a, b) in zip(y_test, predictions3)]  \n",
    "accuracy = (float(sum(map(int, correct))) / float(len(correct)))  \n",
    "print 'Optimized test set accuracy = {0}%'.format(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
